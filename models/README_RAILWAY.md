# ğŸš€ Deploy del Servicio ML - MovieMatch

## ğŸ“‹ Resumen

Este es un microservicio de Machine Learning separado que proporciona predicciones de recomendaciones de pelÃ­culas. EstÃ¡ optimizado para deploy en Railway.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   Backend       â”‚
â”‚   (React)       â”‚            â”‚   (Node.js)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ HTTP
                                       â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   ML Service    â”‚
                              â”‚   (Python)      â”‚
                              â”‚   Railway       â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ ConfiguraciÃ³n para Railway

### 1. Archivos de ConfiguraciÃ³n

- âœ… `railway.json` - ConfiguraciÃ³n de Railway
- âœ… `Procfile` - Comando de inicio
- âœ… `.railwayignore` - Archivos excluidos
- âœ… `requirements.txt` - Dependencias Python

### 2. Optimizaciones Implementadas

#### ğŸš€ Carga Segura del Modelo
```python
# Fallback si el modelo no carga
if model is not None:
    # Usar modelo entrenado
else:
    # Usar algoritmo simple
```

#### ğŸŒ CORS Configurado
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Permitir mÃºltiples aplicaciones
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

#### ğŸ“Š Logging Mejorado
```python
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
```

## ğŸš€ Pasos para Deploy

### 1. Preparar el Repositorio
```bash
# En la carpeta models/
git init
git add .
git commit -m "Initial ML service commit"
```

### 2. Conectar a Railway
```bash
railway login
railway init
```

### 3. Configurar Variables de Entorno
```bash
railway variables set PORT=8000
```

### 4. Deploy
```bash
railway up
```

## ğŸ“¡ Endpoints Disponibles

### Health Check
```http
GET /
GET /health
```

### PredicciÃ³n Individual
```http
POST /predict
Content-Type: application/json

{
  "n_shared_genres": 2,
  "vote_average": 7.5,
  "vote_count": 1000,
  "is_favorite_genre": 1,
  "years_since_release": 3,
  "popularity": 50.0
}
```

### PredicciÃ³n en Lote
```http
POST /predict-batch
Content-Type: application/json

{
  "movies": [
    {
      "movie_id": 1,
      "n_shared_genres": 2,
      "vote_average": 7.5,
      "vote_count": 1000,
      "is_favorite_genre": 1,
      "years_since_release": 3,
      "popularity": 50.0
    }
  ]
}
```

## ğŸ”— IntegraciÃ³n con Backend Principal

### 1. Variable de Entorno
```bash
# En el backend principal
ML_MODEL_URL=https://tu-servicio-ml.railway.app
```

### 2. Uso en el CÃ³digo
```javascript
import { predictMovieRating, predictMultipleMovies } from '../services/mlModelService.js';

// PredicciÃ³n individual
const result = await predictMovieRating(predictionData);

// PredicciÃ³n en lote
const batchResult = await predictMultipleMovies(moviesFeatures);
```

## ğŸ“Š Monitoreo y Logs

### Verificar Estado
```bash
# Ver logs en Railway
railway logs

# Verificar health check
curl https://tu-servicio-ml.railway.app/health
```

### MÃ©tricas Importantes
- âœ… Tiempo de respuesta
- âœ… Uso de memoria
- âœ… Estado del modelo
- âœ… Errores de predicciÃ³n

## ğŸ”„ Escalabilidad

### Ventajas del Servicio Separado
- âœ… **Reutilizable**: MÃºltiples aplicaciones pueden consumirlo
- âœ… **Escalable**: Se puede escalar independientemente
- âœ… **Mantenible**: Actualizaciones sin afectar el backend principal
- âœ… **Especializado**: Optimizado para ML

### Estrategias de Escalado
1. **Horizontal**: MÃºltiples instancias del servicio
2. **Vertical**: MÃ¡s recursos por instancia
3. **Caching**: Cachear predicciones frecuentes
4. **Load Balancing**: Distribuir carga entre instancias

## ğŸ› ï¸ Troubleshooting

### Problemas Comunes

#### 1. Modelo No Carga
```python
# Verificar en logs
logger.error(f"âŒ Error cargando el modelo: {e}")
# El servicio usarÃ¡ algoritmo simple como fallback
```

#### 2. Timeout en Predicciones
```javascript
// Aumentar timeout en el cliente
timeout: 30000 // 30 segundos
```

#### 3. CORS Errors
```python
# Verificar configuraciÃ³n CORS
allow_origins=["*"]  # En desarrollo
```

### Logs Ãštiles
```bash
# Ver logs en tiempo real
railway logs --follow

# Ver logs de errores
railway logs --level error
```

## ğŸ”® PrÃ³ximos Pasos

### Mejoras Futuras
1. **Modelo mÃ¡s ligero**: Usar ONNX o TensorFlow Lite
2. **Caching**: Redis para predicciones frecuentes
3. **A/B Testing**: Comparar modelos
4. **MÃ©tricas**: Prometheus + Grafana
5. **Auto-scaling**: Basado en demanda

### IntegraciÃ³n con Otros Servicios
- âœ… **Google Cloud ML**: Para modelos mÃ¡s complejos
- âœ… **AWS SageMaker**: Para entrenamiento automÃ¡tico
- âœ… **Azure ML**: Para pipelines de ML
- âœ… **Hugging Face**: Para modelos de NLP

## ğŸ“ Soporte

Si tienes problemas:
1. Verifica los logs en Railway
2. Prueba el endpoint `/health`
3. Verifica las variables de entorno
4. Revisa la conectividad de red 